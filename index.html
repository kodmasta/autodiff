<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Autodiff Project Journal</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <h1>Autodiff project</h1>
    <p style="text-align: center;">Weekly updates on my progress implementing the reverse mode of the AD algorithm in Haskell. Project under the supervision of Professor Stefan Monnier.</p>
    <p style="text-align: center;">Project Description: Implement the reverse mode of the AD algorithm based on Haskell's typeclasses, which would work for any program written in Haskell. A literature review on AD and RAD (Randomized Automatic Differentiation).</p>
</header>
<nav>
    <ul>
        <li><a href="#week1">Week 1 (May 13 - May 19)</a></li>
        <li><a href="#week2">Week 2 (May 20 - May 27)</a></li>
        <li><a href="#week3-4">Week 3 - 4 (May 28 - June 10)</a></li>
        <li><a href="#week5-7">Week 5 - 7 (June 11 - July 1)</a></li>
        <li><a href="#week8-9">Week 8 - 9 (July 2 - July 15)</a></li>
        <li><a href="#week10-13">Week 10 - 13 (July 16 - August 5)</a></li>
    </ul>
</nav>
<section id="week1">
    <h2>Week 1 (May 13 - May 19)</h2>
    <p>Read the papers "Automatic differentiation in ML: a Survey" and "Randomized Automatic Differentiation".</p>
    <p>Objective: Understand the math behind AD (accumulating partial derivatives) and investigate what RAD is, and compare with applying normal AD + Stochastic Gradient Descent.</p>
</section>
<section id="week2">
    <h2>Week 2 (May 20 - May 27)</h2>
    <p>Plan: Discuss papers with Professor Stefan Monnier and refamiliarize with functional programming. Watch Youtube tutorials.</p>
    <div class="progress-box">
        <div class="progress-heading">Progress on Literature Review</div>
        <div class="progress-note">In depth review on how AD works mathematically and technically:</div>
        <iframe src="Literature_review_week2.pdf"></iframe>
    </div>
</section>
<section id="week3-4">
    <h2>Week 3 - 4 (May 28 - June 10)</h2>
    <p>Explore the different data structures used to accumulate the derivative. How is the concept of dual numbers applied in code, what is operator overloading, how to code the computation graph of a given haskell program/function.</p>
    <div class="progress-box">
        <div class="progress-heading">Progress on Literature Review</div>
        <div class="progress-note">In depth review on how AD works mathematically and technically part 2:</div>
        <iframe src="Literature_review_week3.pdf"></iframe>
    </div>
    <div class="progress-box">
        <div class="progress-heading">Progress on coding the AD</div>
        <div class="progress-note">AD on a single elementary operation (sumFunc) with \( n\) inputs and \( m\) outputs</div>
        <pre><code>
type R = Double  -- Real numbers
type Vec = [R]   -- Vector of real numbers

-- A function type that includes the function and its gradient
data Func = Func {
  func :: Vec -> Vec,          -- The function
  grad :: Vec -> [Vec]         -- Gradient of the function
}

-- Function f: sums up the inputs and replicates the sum m times
sumFunc :: Int -> Func
sumFunc m = Func {
  func = \xs -> replicate m (sum xs),
  grad = \xs -> replicate m (replicate (length xs) 1)
}

-- Backpropagation algorithm
backprop :: Vec -> Func -> Vec -> [Vec]
backprop yBars f dy_df =
  let
    -- Compute the gradients of f with respect to its inputs
    gradients = grad f yEntire
    -- Initialize yEntire for the first call as yBars
    yEntire = map (sum . zipWith (*) yBars) gradients
  in
    if length yEntire == 1 then [yEntire]
    else yEntire : backprop yEntire (Func (func f) (grad f)) dy_df

-- Example usage:
-- Assuming n = 3 (x1, x2, x3) and m = 4
-- y_i^bar = [1.0, 2.0, 3.0, 4.0]
main :: IO ()
main = do
  let yBars = [1.0, 2.0, 3.0, 4.0]
  let f = sumTableFunc 4
  let dy_df = replicate 4 1  -- Since each y_i is an identity function of the sum
  let results = backprop yBars f dy_df
  print results
        </code></pre>
    </div>
        <div class="progress-box">
            <div class="progress-heading">Questions</div>
            <div class="progress-note"> \(\text{Should all elementary functions be } \mathbb{R}^n \to \mathbb{R} \text{?}
                \) </div>
            <div class="progress-note">How do I break down functions into elementary functions? For example:</div>
            <pre><code>
sumElements :: Vec -> Vec
sumElements xs = let totalSum = sum xs in replicate (length xs) totalSum
            </code></pre>
            <div class="progress-note">How do I accumulate the
                \(\begin{bmatrix}
                \bar{f}_1 \\
                \bar{f}_2 \\
                \vdots \\
                \bar{f}_{k_{x_i}}
                \end{bmatrix} \)
                and
                \( \begin{bmatrix}
                \frac{df_1}{dx_i} \\
                \frac{df_2}{dx_i} \\
                \vdots \\
                \frac{df_{k_{x_i}}}{dx_i}
                \end{bmatrix}
                \)
                inputs for the x layer?</div>


        </div>

</section>

<section id="week5-7">
    <h2>Week 5 - 7 (June 11 - July 1)</h2>
    <p>Code the AD algorithm.</p>
</section>
<section id="week8-9">
    <h2>Week 8 - 9 (July 2 - July 15)</h2>
    <p>Debug, optimize, generalize if possible to more complex class of functions. Try adding stochastic elements in the computation of gradients (RAD)?</p>
</section>
<section id="week10-13">
    <h2>Week 10 - 13 (July 16 - August 5)</h2>
    <p>Finish reading additional papers, complete literature review, and finalize project report.</p>
</section>
<footer>
    <p>Copyright Â© 2024 by Heng Wei</p>
</footer>
</body>
</html>
